{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UgM7VEXCdbTX",
    "outputId": "3e3d2df7-e03c-4aba-d3c9-1275ad0c49cf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1BTTm7E7hz51"
   },
   "outputs": [],
   "source": [
    "url_train_dev = 'https://docs.google.com/spreadsheets/d/1KYIk6fyiTIXe0RkgqNuCvfNnsQtH6lp2fu9bBIorI1s/edit#gid=1591461788'\n",
    "\n",
    "#url_test = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vT-KNR9nuYatLkSbzSRgpz6Ku1n4TN4w6kKmFLkA6QJHTfQzmX0puBsLF7PAAQJQAxUpgruDd_RRgK7/pub?gid=417546901&single=true&output=tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEtmPp7epxEZ"
   },
   "source": [
    "# Load and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "5D8w9zd1c2IJ"
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import requests\n",
    "\n",
    "def load_dataset(url):\n",
    "    r = requests.get(url_train_dev)\n",
    "    data = r.content.decode('utf8')\n",
    "    print(data)\n",
    "    df = pd.read_csv(StringIO(data), sep='\\t')\n",
    "    df.columns = ['bug-id', 'bug', 'label']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "foL-QtlJItoR",
    "outputId": "b234664a-b872-4ebc-ff31-f30a6b995cc1",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 3 fields in line 6, saw 4\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-67c3b3cf52b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:/Users/David/Documents/GitHub/Data_Science_for_Software_Engineering/Project/training_list.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'bug-id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bug'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_train_dev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m7000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1196\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1200\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2155\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2156\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2157\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2158\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2159\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 3 fields in line 6, saw 4\n"
     ]
    }
   ],
   "source": [
    "link = \"C:/Users/David/Documents/GitHub/Data_Science_for_Software_Engineering/Project/training_list.csv\"\n",
    "df = pd.read_csv(link)\n",
    "df.columns = ['bug-id', 'bug', 'label']\n",
    "print(df.head(100))\n",
    "df_train_dev = df[:7000]\n",
    "df_test = df[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWTvXPPgiDzU",
    "outputId": "58b45ee4-6195-4fda-f8ee-ad36a88ef6c6"
   },
   "outputs": [],
   "source": [
    "print('Infos train-dev-set:')\n",
    "print(df_train_dev.info())\n",
    "print('Infos test-set:')\n",
    "print(df_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "h8lCjkb_kSOp",
    "outputId": "53afd811-d87c-47a0-a097-486927bf19c1"
   },
   "outputs": [],
   "source": [
    "df_train_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-K1HUEordhz",
    "outputId": "2b8d17c7-2175-4319-abd7-99f06d88e3b3"
   },
   "outputs": [],
   "source": [
    "print(df_train_dev.label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "mok54OinmXQw",
    "outputId": "19408f86-6111-403b-cd7f-9939b3d3eb70"
   },
   "outputs": [],
   "source": [
    "df_train_dev.groupby('label').size().sort_values(ascending = False).plot.bar(figsize=(20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f24YMQAz44u",
    "outputId": "dcbfaa26-7282-4c2d-adc5-4200126d9ab2"
   },
   "outputs": [],
   "source": [
    "df_train_dev.groupby('label').size().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qxb9qeRprB8"
   },
   "source": [
    "# Process labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FwgxNUdngK1"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_fitted = LabelEncoder().fit(df_train_dev['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2V6JkOLrn-75",
    "outputId": "bc49349d-2fd2-483d-a8ec-725f0a6cbffe"
   },
   "outputs": [],
   "source": [
    "# map all classes that are not in train_dev to undefined\n",
    "for i, label in enumerate(df_test['label']):\n",
    "    df_test['label'][i] = 'und' if label not in le_fitted.classes_ else label\n",
    "# check if it worked: should return an empty list\n",
    "print([label for label in df_test['label'] if label not in set(df_train_dev['label'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "Y4mGBukF1Dzu",
    "outputId": "77edc40d-ed9f-4a69-e5af-65b0f97d68c1"
   },
   "outputs": [],
   "source": [
    "df_train_dev['label'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EV6KmRn4oOIF"
   },
   "outputs": [],
   "source": [
    "y_train_dev, y_test = le_fitted.transform(df_train_dev['label']), le_fitted.transform(df_test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0gShv_UsLiZ"
   },
   "source": [
    "# Preprocess bugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93UMrd5BtL-B"
   },
   "source": [
    "Pipeline classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6iOls7ib_-H"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "class TweetNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "\n",
    "    def _normalize_tweet(self, tweet):\n",
    "        \"\"\"Remove punctuation and newlines, lowercase, pad with spaces.\n",
    "\n",
    "        :param tweet: string\n",
    "        :return: normalized string\n",
    "        \"\"\"\n",
    "        tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "        tweet = re.sub(r'\\n', r'', tweet)\n",
    "        tweet = tweet.lower()\n",
    "        tweet = re.sub(r'@\\w+\\b', r'', tweet)\n",
    "        tweet = re.sub(r'\\b\\S+//\\S+\\b', r'', tweet)\n",
    "        # tweet = ' ' + tweet + ' '\n",
    "        return tweet\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        tweets = []\n",
    "        for tweet in X:\n",
    "            tweets.append(self._normalize_tweet(tweet))\n",
    "        return np.array(tweets)\n",
    "\n",
    "\n",
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "\n",
    "    vowels = set([c for c in 'aeiouäöüàéèëï'])\n",
    "    consonants = set([c for c in 'bcdfghklmnlpqrstvwxyz'])\n",
    "\n",
    "    def __init__(self):\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def _to_bigrams(self, tweet):\n",
    "        return [bg[0] + bg[1] for bg in zip(tweet, tweet[1:])]\n",
    "\n",
    "    def _get_vowel_consonant_ratio(self, tweet):\n",
    "        vf = 0\n",
    "        cf = 0\n",
    "        for c in tweet.lower():\n",
    "            if c in self.vowels:\n",
    "                vf =+ 1\n",
    "            elif c in self.consonants:\n",
    "                cf += 1\n",
    "        return vf / (cf + 1)\n",
    "\n",
    "    def _get_capitalization_ratio(self, tweet):\n",
    "        up_count = 0\n",
    "        for c in tweet:\n",
    "            if c.upper() == c:\n",
    "                up_count += 1\n",
    "        return up_count / (len(tweet) + 1)\n",
    "\n",
    "    def _get_double_char_freq(self, tweet):\n",
    "        double_freq = 0\n",
    "        for bg in self._to_bigrams(tweet):\n",
    "            if bg[0] == bg[1]:\n",
    "                double_freq += 1\n",
    "        return double_freq\n",
    "    \n",
    "    def _extract_num_features(self, tweets):\n",
    "        num_features = []\n",
    "        for tweet in tweets:\n",
    "            feat_tweet = []\n",
    "            feat_tweet.append(self._get_vowel_consonant_ratio(tweet))\n",
    "            feat_tweet.append(self._get_capitalization_ratio(tweet))\n",
    "            feat_tweet.append(self._get_double_char_freq(tweet))\n",
    "            num_features.append(feat_tweet)\n",
    "        return np.array(num_features)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        numerical_features = self._extract_num_features(X)\n",
    "        self.scaler.fit(numerical_features)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        numerical_features= self._extract_num_features(X)\n",
    "        return X, self.scaler.transform(numerical_features)\n",
    "\n",
    "\n",
    "class MatrixToArrayConverter1(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[0].toarray(), X[1]\n",
    "\n",
    "\n",
    "class MatrixUnifier(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return np.concatenate([X[0].todense(), X[1]], axis=1)\n",
    "\n",
    "\n",
    "class CountVectorizerWrapper:\n",
    "\n",
    "    def __init__(self, ngram_range, analyzer, max_features, binary):\n",
    "        print('args:', str([ngram_range, analyzer, max_features, binary]))\n",
    "        self.countvec = CountVectorizer(ngram_range=ngram_range, analyzer=analyzer, max_features=max_features, binary=binary)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        tweets, numerical_features = X\n",
    "        self.countvec.fit(tweets)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        tweets, numerical_features = X\n",
    "        return self.countvec.transform(tweets), numerical_features\n",
    "\n",
    "\n",
    "class OneHotEncoderWrapper:\n",
    "\n",
    "    def __init__(self, handle_unknown):\n",
    "        self.ohe = OneHotEncoder(handle_unknown=handle_unknown)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.ohe.fit(X[0])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return self.ohe.transform(X[0]), X[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3gvmuUPtS5j"
   },
   "source": [
    "Helper classes for the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asuv2Ey8suTF"
   },
   "outputs": [],
   "source": [
    "class GenericClassifier(BaseEstimator):\n",
    "\n",
    "    def __init__(self, estimator):\n",
    "        self.clf = clf\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        self.clf.fit(X, y)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        return self.clf.predict(X)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.clf.predict_proba(X)\n",
    "\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.clf.score(X, y)\n",
    "\n",
    "\n",
    "class Debug(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X):\n",
    "        import pdb; pdb.set_trace()\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yerRC06qtabm"
   },
   "source": [
    "# GridSearch and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQq5eigQKi2D"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pDxK9slayq3"
   },
   "outputs": [],
   "source": [
    "clf_param_grid = {\n",
    "    'MultinomialNB': [MultinomialNB, {'CLF__alpha': [0.1, 1]}],\n",
    "    'SGDClassifier': [SGDClassifier, {'CLF__loss': ['hinge', 'log'], 'CLF__penalty': ['l2', 'l1'], 'CLF__max_iter': [100, 300], 'CLF__early_stopping': [True, False]}]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YOsRd7OoFUWn",
    "outputId": "1f616e78-2bbd-42bc-febe-97dd7bd2b30c"
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for clf_name in clf_param_grid:\n",
    "    print(30*'-')\n",
    "    print(clf_name)\n",
    "    param_grid = clf_param_grid[clf_name][1]\n",
    "    print(param_grid)\n",
    "    bigram_vec_args = dict(ngram_range=(2,2), analyzer='char_wb', max_features=100, binary=True)\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('TweetNormalizer', TweetNormalizer()),\n",
    "        ('FeatureExtractor', FeatureExtractor()),\n",
    "        ('BigramVectorizer', CountVectorizerWrapper(**bigram_vec_args)),\n",
    "        ('MatrixToArrayConverter', MatrixToArrayConverter1()),\n",
    "        ('OneHotEncoder', OneHotEncoderWrapper(handle_unknown='ignore')),\n",
    "        ('MatrixUnifier', MatrixUnifier()),\n",
    "        ('CLF', clf_param_grid[clf_name][0]())\n",
    "    ], verbose=True)\n",
    "    grid = GridSearchCV(pipe, n_jobs=1, param_grid=param_grid, scoring='f1_micro', cv=10)\n",
    "    grid.fit(df_train_dev['bug'].to_numpy(), y_train_dev)\n",
    "    models.append(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aA3JAYJBDZxy"
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZHDby7NB7PW"
   },
   "source": [
    "Micro f1-Score of the naive base models on the dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDOlBtb2B4Nq",
    "outputId": "0389548c-2721-41d4-dc94-2df01b8de92c"
   },
   "outputs": [],
   "source": [
    "models[0].cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcqrfTK1Bcxd"
   },
   "source": [
    "Micro and macro f1-score of the best naive bayes model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIlAIaVnNHOU",
    "outputId": "a0323b54-78ef-42eb-c0b9-64f6e45ae3b1"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "preds = models[0].predict(df_test['bug'].to_numpy())\n",
    "f1_micro = f1_score(preds, y_test, average='micro')\n",
    "f1_macro = f1_score(preds, y_test, average='macro')\n",
    "print(f'F1-micro-score on the testset: {f1_micro}')\n",
    "print(f'F1-macro-score on the testset: {f1_macro}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RKO1bbsCDne"
   },
   "source": [
    "Micro f1-Score of the SGD models on the dev set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FW-5WG9SBzyU",
    "outputId": "e5bde2da-07eb-487c-99fd-9f1b0b3c40e9"
   },
   "outputs": [],
   "source": [
    "models[1].cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlY-SAdYBmty"
   },
   "source": [
    "Accuracy of the best SGD model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PEm-z0w3NiBf",
    "outputId": "170c4845-ac41-481f-edf7-bb14749328e8"
   },
   "outputs": [],
   "source": [
    "preds = models[1].predict(df_test['bug'].to_numpy())\n",
    "f1_micro = f1_score(preds, y_test, average='micro')\n",
    "f1_macro = f1_score(preds, y_test, average='macro')\n",
    "print(f'F1-micro-score on the testset: {f1_micro}')\n",
    "print(f'F1-macro-score on the testset: {f1_macro}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kqDIw95Nsr2"
   },
   "source": [
    "Let's check the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "Xeb6jHmeAeVK",
    "outputId": "30e164f5-f8b6-43f5-b02d-c5fac3747d8c"
   },
   "outputs": [],
   "source": [
    "num_classes = len(le_fitted.classes_)\n",
    "def create_confusion_matrix(num_classes, preds, y_test):\n",
    "    \"\"\"Create confusion matrix 'by hand' since test set does not contain all labels (thanks to Sarah Kiener).\"\"\"\n",
    "    df = pd.DataFrame(np.zeros((num_classes, num_classes), dtype=int))\n",
    "    for i, j in zip(preds, y_test):\n",
    "        df.iloc[i, j] += 1\n",
    "    df.columns = le_fitted.classes_\n",
    "    df.index = le_fitted.classes_\n",
    "    return df\n",
    "df = create_confusion_matrix(num_classes, preds, y_test)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dataseminar.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
